{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ff525a-e72f-4e00-a381-8040519bd55f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lectures 7: Class demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca5e2b-94aa-49d8-9810-1b03861840e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../code/.\")\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotting_functions import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda548e-7def-4519-b10e-8458f972a98f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo: Model interpretation of linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a3bbe-696f-49a8-bc59-cfdf60fd3a5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- One of the primary advantages of linear classifiers is their ability to interpret models.\n",
    "- For instance, by analyzing the sign and magnitude of the learned coefficients, we can address questions regarding which features are influencing the prediction and in which direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b5df2-0ca5-4c7d-9e85-c349245f8c13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll demonstrate this by training `LogisticRegression` on the famous [IMDB movie review](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) dataset. The dataset is a bit large for demonstration purposes. So I am going to put a big portion of it in the test split to speed things up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7fba1-5649-4bf5-acd9-ecca9a9c6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv(\"../data/imdb_master.csv\", encoding=\"ISO-8859-1\")\n",
    "imdb_df = imdb_df[imdb_df[\"label\"].str.startswith((\"pos\", \"neg\"))]\n",
    "imdb_df.drop([\"Unnamed: 0\", \"type\", \"file\"], axis=1, inplace=True)\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d958f04-51d1-40a2-80ce-8f257800c62e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's clean up the data a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5a105-6f0f-4167-84ae-32286782a1b8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def replace_tags(doc):\n",
    "    doc = doc.replace(\"<br />\", \" \")\n",
    "    doc = re.sub(\"https://\\S*\", \"\", doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184db51-7841-4510-9399-9d4c66a24efd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "imdb_df[\"review_pp\"] = imdb_df[\"review\"].apply(replace_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bd860-2f05-4a4b-8377-9bae3b53d1d4",
   "metadata": {},
   "source": [
    "Are we breaking the Golden rule here? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbba06e-68af-4344-9b9f-9ad5ab9885cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's split the data and create bag of words representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a4ed9-6ebd-46c3-ba94-4ae27c58f460",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(imdb_df, test_size=0.9, random_state=123)\n",
    "X_train, y_train = train_df[\"review_pp\"], train_df[\"label\"]\n",
    "X_test, y_test = test_df[\"review_pp\"], test_df[\"label\"]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa1f28-9f04-4e45-9468-98257404b4cd",
   "metadata": {},
   "source": [
    "Is there any missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b08daf-4b68-491f-8117-c59745aaeccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf70e7-cbec-4511-b38b-ba20e4607257",
   "metadata": {},
   "source": [
    "There is no missing data. We don't need imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83302a83-2fe7-4885-9181-c6b5b1dda33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try CountVectorizer\n",
    "vec = None\n",
    "bow = None\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31c51a-b6df-46c6-aaea-c66b1ea88c39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examining the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4cac6-d789-4da2-81e7-33ef7978a838",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The vocabulary (mapping from feature indices to actual words) can be obtained using `get_feature_names()` on the `CountVectorizer` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2e485-b06f-4b53-9626-075d644db027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193df8b7-5185-4bfd-88bc-b29814b59e0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# vocab[0:10]  # first few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f9cf8-6f84-408e-9822-83e805d43700",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# vocab[2000:2010]  # some middle words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ea432-355d-4f4f-94f3-0097c390a73c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# vocab[::500]  # words with a step of 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05edd7-3b6f-4431-bd5b-daa0037a4dce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model building on the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab803512-7f24-4fa2-bd2d-e85c292a82f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First let's try `DummyClassifier` on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08438e8-f3cf-44f2-8021-dcdfae0a4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()\n",
    "cross_val_score(dummy, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d858b-48ba-40c3-8563-85994e59c1eb",
   "metadata": {},
   "source": [
    "We have a balanced dataset. So the `DummyClassifier` score is around 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bcfaf-d656-4eca-b693-11c3e7308b03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's try logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc691c78-f1b8-4b08-b398-071c267c94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with CountVectorizer and LogisticRegression \n",
    "pipe_lr = None\n",
    "# scores = cross_validate(pipe_lr, X_train, y_train, return_train_score=True)\n",
    "# pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72469c0-d6b1-4e70-84e8-a1d382c363f6",
   "metadata": {},
   "source": [
    "Seems like we are overfitting. Let's optimize the hyperparameter `C`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108ec94-255f-4d39-b4ab-ccc5750f9e96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scores_dict = {\n",
    "    \"C\": 10.0 ** np.arange(-3, 3, 1),\n",
    "    \"mean_train_scores\": list(),\n",
    "    \"mean_cv_scores\": list(),\n",
    "}\n",
    "for C in scores_dict[\"C\"]:\n",
    "    pipe_lr = None\n",
    "    # scores = cross_validate(pipe_lr, X_train, y_train, return_train_score=True)\n",
    "    # scores_dict[\"mean_train_scores\"].append(scores[\"train_score\"].mean())\n",
    "    # scores_dict[\"mean_cv_scores\"].append(scores[\"test_score\"].mean())\n",
    "\n",
    "# results_df = pd.DataFrame(scores_dict)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fcf649-d025-4570-9278-98d0cd87e54a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# optimized_C = results_df[\"C\"].iloc[np.argmax(results_df[\"mean_cv_scores\"])]\n",
    "# print(\n",
    "#     \"The maximum validation score is %0.3f at C = %0.2f \"\n",
    "#     % (\n",
    "#         np.max(results_df[\"mean_cv_scores\"]),\n",
    "#         optimized_C,\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba72239-0329-456c-837e-ccd655bdd8ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train a model on the full training set with the optimized hyperparameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8913c96-7707-417c-8d98-c5c7c75f7390",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e573adc-11b8-481f-91ad-93213180aff0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examining learned coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a2eb9-8b8c-4283-b7ca-8af5fc58649a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The learned coefficients are exposed by the `coef_` attribute of [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2be41c-d208-4284-bc56-17f23b5b0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = np.array(pipe_lr.named_steps[\"countvectorizer\"].get_feature_names_out())\n",
    "# coeffs = pipe_lr.named_steps[\"logisticregression\"].coef_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7203a8-4c25-4e4e-a75a-90b7ab6eb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2fc7e-8bb0-499b-b329-97ec1124e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_coeff_df = pd.DataFrame(coeffs, index=feature_names, columns=[\"Coefficient\"])\n",
    "# word_coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d4237-22a5-43de-b849-e25245e426b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's sort the coefficients in descending order. \n",
    "- Interpretation\n",
    "    - if $w_j > 0$ then increasing $x_{ij}$ moves us toward predicting $+1$. \n",
    "    - if $w_j < 0$ then increasing $x_{ij}$ moves us toward predicting $-1$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0fef3-deec-4ad2-8d12-2c26f1e49553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_coeff_df.sort_values(by=\"Coefficient\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923950f-a77f-46a3-aaa2-2201208f835c",
   "metadata": {},
   "source": [
    "- The coefficients make sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769e763-2d74-416f-b0a7-258d8ab95599",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize the top 20 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c630f6-ddbf-4c47-a1c4-4fecd2473a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mglearn.tools.visualize_coefficients(coeffs, feature_names, n_top_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20bea3-925a-43a5-9433-bb769c0b8c03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explore prediction of the following new review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e77299-2aae-43a7-b0af-30e06eb8b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_review = \"It got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b1954-119e-41de-8683-b2796b07166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_vec = pipe_lr.named_steps[\"countvectorizer\"].transform([fake_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882de7f-d046-41db-a9be-6dd2f02043b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca844028-a170-44ee-977c-d890e6021eb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get prediction probability scores of the fake review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359de100-ac47-4c6e-88f9-05222717fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_lr.predict_proba([fake_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2360f6-3895-4b54-a6af-8f580f1d2b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_lr.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b2863-0d1d-4440-b553-62eba0c78db4",
   "metadata": {},
   "source": [
    "The model is 83.5% confident that it's a positive review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc59d2c-3921-4798-be46-90f6abe90507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_lr.predict([fake_review])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fde22-01b8-4a14-a55e-0f35abc92c2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can find which of the vocabulary words are present in this review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c6d99-506a-4dd8-9188-f5fbd2ca2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_vec.toarray().ravel().astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb012e-05f1-4e00-85da-5d0c3ca1b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_in_ex = feat_vec.toarray().ravel().astype(bool)\n",
    "# words_in_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01120ebb-68ef-4eda-925e-43b349352db5",
   "metadata": {},
   "source": [
    "How many of the words are in this review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0012438-3471-40c5-bf3b-23ae05e7c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(words_in_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463a1a6-c660-4f6b-afe4-7ddebb14c34c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# np.array(feature_names)[words_in_ex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384edca-331d-4e83-968f-a62053a1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex_df = pd.DataFrame(\n",
    "#     data=coeffs[words_in_ex],\n",
    "#     index=np.array(feature_names)[words_in_ex],\n",
    "#     columns=[\"Coefficient\"],\n",
    "# )\n",
    "# ex_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1948622-5475-401b-bceb-a4e897f5877e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize how the words with positive and negative coefficients are driving the hard prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880af16d-fa78-468b-8ea0-5dc94a9bdca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mglearn.tools.visualize_coefficients(\n",
    "#     coeffs[words_in_ex], np.array(feature_names)[words_in_ex], n_top_features=6\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98580e75-2fe3-4176-a811-91e118648c29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# def plot_coeff_example(feat_vect, coeffs, feature_names):\n",
    "#     words_in_ex = feat_vec.toarray().ravel().astype(bool)\n",
    "\n",
    "#     ex_df = pd.DataFrame(\n",
    "#         data=coeffs[words_in_ex],\n",
    "#         index=np.array(feature_names)[words_in_ex],\n",
    "#         columns=[\"Coefficient\"],\n",
    "#     )\n",
    "#     return ex_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed424aae-0a13-45f5-8002-aca241e707db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Most positive review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac2f69-137f-4154-b4cc-8fbb9bf47d05",
   "metadata": {},
   "source": [
    "- Remember that you can look at the probabilities (confidence) of the classifier's prediction using the `model.predict_proba` method.\n",
    "- Can we find the reviews where our classifier is most confident or least confident?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a61595-a835-40fc-9ea6-d462d0b5b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_probs = pipe_lr.predict_proba(X_train)[\n",
    "#     :, 1\n",
    "# ]  # only get probabilities associated with pos class\n",
    "# pos_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d655755-b6f3-4671-b352-21d1e93a8f1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get the index of the example where the classifier is most confident (highest `predict_proba` score for positive). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd679117-7da8-49c8-aa67-772de50b53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_positive = np.argmax(pos_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a788d1c-85be-40ce-9e8b-d664047dc09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.iloc[most_positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5516c-4b35-418f-96cc-b157f5584d7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"True target: %s\\n\" % (y_train.iloc[most_positive]))\n",
    "# print(\"Predicted target: %s\\n\" % (pipe_lr.predict(X_train.iloc[[most_positive]])[0]))\n",
    "# print(\"Prediction probability: %0.4f\" % (pos_probs[most_positive]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d92fd7-06b0-4d22-b81d-d9c5a4a1cbcd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine the features associated with the review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f9a00-3714-4406-97ce-bcf0637d3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_vec = pipe_lr.named_steps[\"countvectorizer\"].transform(\n",
    "#     X_train.iloc[[most_positive]]\n",
    "# )\n",
    "# words_in_ex = feat_vec.toarray().ravel().astype(bool)\n",
    "# mglearn.tools.visualize_coefficients(\n",
    "#     coeffs[words_in_ex], np.array(feature_names)[words_in_ex], n_top_features=20\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74401c-9890-48e9-a0da-411ee905ce81",
   "metadata": {},
   "source": [
    "The review has both positive and negative words but the words with **positive** coefficients win in this case! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f0d0e-c685-4c03-9bc6-44d3aefd2142",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Most negative review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabb5bc-f636-4bb7-b11c-0981f02fada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_probs = pipe_lr.predict_proba(X_train)[\n",
    "#     :, 0\n",
    "# ]  # only get probabilities associated with pos class\n",
    "# neg_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52985c78-a5ee-427b-8560-9b59f7e0418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_negative = np.argmax(neg_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0f5ca-a9ba-440b-8c30-86436a0576a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Review: %s\\n\" % (X_train.iloc[[most_negative]]))\n",
    "# print(\"True target: %s\\n\" % (y_train.iloc[most_negative]))\n",
    "# print(\"Predicted target: %s\\n\" % (pipe_lr.predict(X_train.iloc[[most_negative]])[0]))\n",
    "# print(\"Prediction probability: %0.4f\" % (pos_probs[most_negative]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8a612-bb9d-4004-b5be-d3c5b167437f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# feat_vec = pipe_lr.named_steps[\"countvectorizer\"].transform(\n",
    "#     X_train.iloc[[most_negative]]\n",
    "# )\n",
    "# words_in_ex = feat_vec.toarray().ravel().astype(bool)\n",
    "# mglearn.tools.visualize_coefficients(\n",
    "#     coeffs[words_in_ex], np.array(feature_names)[words_in_ex], n_top_features=20\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc03c8-c87f-4f27-a74d-46397c98fa15",
   "metadata": {},
   "source": [
    "The review has both positive and negative words but the words with negative coefficients win in this case! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b09e7-f5c8-459f-9785-8b9a94d169ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d9784-d5aa-4751-8642-739c50711083",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "#### Question for you to ponder on \n",
    "\n",
    "- Is it possible to identify most important features using $k$-NNs? What about decision trees?  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "cpsc330"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
